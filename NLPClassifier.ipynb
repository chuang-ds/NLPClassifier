{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for Natural Language Processing\n",
    "\n",
    "### Chet Huang, 12 Jan 2020\n",
    "\n",
    "In this notebook we explore Natural Language Processing using recurrent neural networks.  The dataset we use is the Keras Reuters newswires dataset. This dataset contains 11,228 Reuters newswires, each classified as one of 46 topics.  In this notebook we train and test various RNN/LSTM models to recognize newswire topics using sequences of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Tensorflow, Keras, and Numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras contains a Reuters dataset which we import below.  We set a top limit of 4000 most popular words and load the data into training (75%) and test sets (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "top_words = 4000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\", num_words=top_words, skip_top=0, maxlen=None,\n",
    "                                                         test_split=0.25, seed=113, start_char=1, oov_char=2, index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample newswire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 599, 1815, 299, 45, 1320, 194, 198, 2041, 28, 932, 294, 517, 1213, 2, 334, 66, 199, 8, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the newswire is expressed in numbers and not words.  The words are encoded as a sequence of word indexes where words are indexed by overall frequency. For example, \"8\" encodes the 8th most frequent word in the data.  Let's build a function to decode these newswires into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_newswire(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now decode the sample newswire using our decode function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> french operators did not request any export licences at today's european community weekly <UNK> tender trade sources said reuter 3\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_newswire(x_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to transform the newswires so that they all have the same length for modeling. For this, we will use the preprocessing library within Keras.  The preprocessing can either trim newswires to a max number of words or pad newswires with essentially blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider the first 200 words within a newswire\n",
    "max_newswire_length = 200\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_newswire_length, truncating='post')\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_newswire_length, truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to build our initial model. We will have sequential layers with an input layer, an LSTM layer, and dense output layer. The input layer is of size 32 for each input word. The second layer is an LSTM layer with size 100, and finally 46 output nodes since newswires can be any of 46 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 32)           128000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 185,846\n",
      "Trainable params: 185,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8421 samples\n",
      "Epoch 1/5\n",
      "8421/8421 [==============================] - 41s 5ms/sample - loss: 2.6855 - accuracy: 0.3513\n",
      "Epoch 2/5\n",
      "8421/8421 [==============================] - 39s 5ms/sample - loss: 2.4204 - accuracy: 0.3536\n",
      "Epoch 3/5\n",
      "8421/8421 [==============================] - 39s 5ms/sample - loss: 2.3916 - accuracy: 0.3536\n",
      "Epoch 4/5\n",
      "8421/8421 [==============================] - 39s 5ms/sample - loss: 2.3561 - accuracy: 0.3536\n",
      "Epoch 5/5\n",
      "8421/8421 [==============================] - 39s 5ms/sample - loss: 2.2473 - accuracy: 0.3547\n",
      "\n",
      "Model accuracy with test data: 39.72%\n"
     ]
    }
   ],
   "source": [
    "#### Construct our model\n",
    "embedding_vector_length = 32\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(top_words, embedding_vector_length, input_length=max_newswire_length))\n",
    "model.add(keras.layers.LSTM(100))\n",
    "model.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "\n",
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"\\nModel accuracy with test data: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial model achieved ~35% accuracy.  Let's try changing the activation function in the LSTM layer to tanh and see if we can get a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 32)           128000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 185,846\n",
      "Trainable params: 185,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8421 samples\n",
      "Epoch 1/5\n",
      "8421/8421 [==============================] - 40s 5ms/sample - loss: 2.5255 - accuracy: 0.3590\n",
      "Epoch 2/5\n",
      "8421/8421 [==============================] - 39s 5ms/sample - loss: 2.0218 - accuracy: 0.4805\n",
      "Epoch 3/5\n",
      "8421/8421 [==============================] - 39s 5ms/sample - loss: 1.8347 - accuracy: 0.5242\n",
      "Epoch 4/5\n",
      "8421/8421 [==============================] - 38s 5ms/sample - loss: 1.7757 - accuracy: 0.5274\n",
      "Epoch 5/5\n",
      "8421/8421 [==============================] - 38s 5ms/sample - loss: 1.7434 - accuracy: 0.5340\n",
      "\n",
      "Model accuracy with test data: 56.43%\n"
     ]
    }
   ],
   "source": [
    "#### Construct our model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(top_words, embedding_vector_length, input_length=max_newswire_length))\n",
    "model.add(keras.layers.LSTM(100, activation='tanh'))\n",
    "model.add(keras.layers.Dense(46, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "\n",
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"\\nModel accuracy with test data: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second model achieved accuracy ~55%.  Using tanh activation did not seem to help improve model accuracy.  Let's try adding more LSTM layers and see if a deeper network can help improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 32)           128000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200, 46)           14536     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200, 92)           51152     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 138)               127512    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                6394      \n",
      "=================================================================\n",
      "Total params: 327,594\n",
      "Trainable params: 327,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8421 samples\n",
      "Epoch 1/10\n",
      "8421/8421 [==============================] - 115s 14ms/sample - loss: 2.2093 - accuracy: 0.4381\n",
      "Epoch 2/10\n",
      "8421/8421 [==============================] - 112s 13ms/sample - loss: 1.7028 - accuracy: 0.5572\n",
      "Epoch 3/10\n",
      "8421/8421 [==============================] - 112s 13ms/sample - loss: 1.6296 - accuracy: 0.5851\n",
      "Epoch 4/10\n",
      "8421/8421 [==============================] - 111s 13ms/sample - loss: 1.5376 - accuracy: 0.6082\n",
      "Epoch 5/10\n",
      "8421/8421 [==============================] - 111s 13ms/sample - loss: 1.4917 - accuracy: 0.6170\n",
      "Epoch 6/10\n",
      "8421/8421 [==============================] - 112s 13ms/sample - loss: 1.3871 - accuracy: 0.6434\n",
      "Epoch 7/10\n",
      "8421/8421 [==============================] - 111s 13ms/sample - loss: 1.4179 - accuracy: 0.6452\n",
      "Epoch 8/10\n",
      "8421/8421 [==============================] - 111s 13ms/sample - loss: 1.4387 - accuracy: 0.6247\n",
      "Epoch 9/10\n",
      "8421/8421 [==============================] - 112s 13ms/sample - loss: 1.2817 - accuracy: 0.6689\n",
      "Epoch 10/10\n",
      "8421/8421 [==============================] - 112s 13ms/sample - loss: 1.1798 - accuracy: 0.6964\n",
      "\n",
      "Model accuracy with test data: 64.87%\n"
     ]
    }
   ],
   "source": [
    "# Construct our model\n",
    "model = keras.models.Sequential()\n",
    "embedding_vector_length = 32\n",
    "model.add(keras.layers.Embedding(top_words, embedding_vector_length, input_length=max_newswire_length))\n",
    "model.add(keras.layers.LSTM(46, return_sequences=True, input_shape=(8, max_newswire_length), activation='tanh'))\n",
    "model.add(keras.layers.LSTM(92, return_sequences=True, activation='tanh'))\n",
    "model.add(keras.layers.LSTM(138))\n",
    "model.add(keras.layers.Dense(46, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"\\nModel accuracy with test data: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we learned how to load Reuters newswires from Keras dataset, pad and trim text to a constant length, and build numerous RNN/LSTM models to predict the newswire category.  We found that ~65% prediction accuracy can be achieved after 10 training epochs.  The trend of model performance by training epoch suggests even better model performance can be achieved with more training though 10 epochs is sufficient to demonstrate RNN/LSTM capabilities for natural language processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
